\section{Обзор предметной области}
    % Объяснить, какие подходы к классификации рассматриваются в этой области.
    На сегодняшний день, большинство пользователей крупных социальных сетей,
    таких как \twitter, предпочитают высказывать свои отзывы и мнения о товарах
    и услугах в формате коротких сообщений. Возможность быстрого реагирования
    на сообщения со стороны компаний, предоставляемых эти услуги, возможно лишь
    в случае их автоматической обработки.

    Как известно, одним из направлений в решении задачи классификации является
    использование методов машинного обучения. Ввиду существенного роста объема
    доступной информации социальных сетей, такие проблемы как адаптация и
    обучение классификационных моделей становятся все менее значительными.
    В связи с этим, рассмотрим наиболее популярные методы машинного обучения \cite{svmAdvantages},
    которые находят свое применение к задачи тональной классификации сообщений.

    \subsection{Подходы к тональной классификации на основе методов машинного обучения}
        \label{sec:mlApproaches}
        Основой работы рассматриваемых методов является представление исходных
        сообщений $m$ в формате вектора нормализованых слов $\{F_1, F_2, \ldots, F_n\}$.
        В качеcтве значений каждой из размерности вектора $\vec{m}$ можно сопоставить
        $n_{F_i}(m)$ -- число вхождений терма $F_i$ в рассматриваемое сообщение $m$.
        Такая модель является вариацией {\it Bag Of Words} \cite{svmAdvantages}.
        В результате, сообщение $m$ представляет собой вектор:
        \begin{equation}
            \vec{m} = (n_1(m), n_2(m), \ldots, n_k(m))
        \end{equation}

        \subsubsection{Метод <<Наивного Байеса>>}
        % На основе статьи
        Один из подходов к класификации сообщений заключается в определении класса $c^{*}$,
        к которому относится рассматриваемое сообщение $m$ на основе метрики
        {\it максимального правдоподобия}:
        \begin{equation}
            c^{*} = argmax_c \hspace{2pt} P(c|m)
        \end{equation}

        В основе вычисления условной вероятности $P(c|m)$ лежит правило Байеса:
        \begin{equation}
            \label{eq:BayesRule}
            P(c|m) = \dfrac{P(c, m)}{P(m)} = \dfrac{P(c)\cdot P(m|c)}{P(m)}
        \end{equation}

        Классификатор, построенный на основе правила, представленного в
        формуле \ref{eq:BayesRule}, называется {\it NB}-классификатором.
        Для оценки условной вероятности в формуле \ref{eq:BayesRule}, предполагается
        независимость термов сообщения, и вычисляется следующим образом:
        \begin{equation}
            P_{NB} (c|m) = \dfrac{P(c)\cdot(\Pi_{i=1}^{n}P(F_i|c)^{n_i(m)})}{P(m)} \nonumber
        \end{equation}

        Несмотря на простоту реализации алгоритма, независимость термов $F_i$ сообщения
        является ограничением для достижения реального правдоподобия. Работа \cite{nbAdvantages}
        демонстрирует оптимальность метода Наивного Байеса для большинства задач
        классификации, в которых присутствуют признаки, позволяющие установить
        тесную связь с соответствующими классами. В тоже время, использование
        более сложных методов позволяет добиться лучших результатов.

        %\subsubsection{Метод <<Максимальной энтропии>>}
        % Из статьи
        \subsubsection{Метод <<Опорных векторов>>}
        % 9.1.2
        В отличие от метода <<Наивного Байеса>>, подход на основе рассматриваемого
        метода предполагает поиск гиперплоскости, разделяющей сообщения разных классов.
        Построение выполняется на этапе обучения модели. На этом этапе решается задача
        поиска нормали $\vec{w}$ к гиперплоскости, причем разбиение классов должно
        производиться с максимально возможным отступом.

        Для поиска нормали составляется {\it оптимизационная задача с граничными
        условиями}:
        \begin{equation}
            \label{eq:optimizationSVM}
            \vec{w} = \sum_{j=1}^{N} \alpha_i c_i \vec{m_i}, \hspace{2pt} \alpha_i \geq 0
        \end{equation}

        В уравнении \ref{eq:optimizationSVM}, коэффициент $c_i \in \{-1, 1\}$
        указывает на принадлежность сообщения $m_i$ соответствующему классу;
        $\alpha_i$ -- коээфициент решения задачи двойной оптимизации. Среди всех
        векторов $\vec{m}$, для которых выполнено условие $\alpha_i > 0$, называются <<опорными>>.
        Определения класса, к которому относится рассматриваемый документ, осуществляется
        на основе стороны гиперплоскости, на которую падает проекция вектора $\vec{m}$.

        В общем случае, классификатор построенный на рассматриваемом подходе,
        позволяет достичь лучших резульатов классификации если сравнивать с
        аналогом на основе метода <<Наивного Байеса>> \cite{svmCompareVsNB}.

        (Дополнить на основе аналогичного раздела из \cite{islr})

        % SVM with multiple classes
    \subsection{Признаки, используемые для классификации сообщений}
        % Лемматизация термов сообщения на основе метрик
        \subsubsection{Векторизация сообщений}
        % bag of words
        Под термином {\it сообщение} будем понимать вектор, состоящий из
        нормализованных слов -- {\it термов}. Векторизация сообщения -- процесс
        преобразования сообщения в вектор, в котором каждому терму сообщения
        сопоставляется некоторое числовое значение.

        В п. \ref{sec:mlApproaches} рассматривался один из самых простых
        способов векторизации сообщений: {\it Bag Of Words}. Помимо факта присутствия
        терма и числа его вхождения в сообщение, такая векторизация не несет в
        себе никакой дополнительной информации терма относительно всей коллекции
        сообщений.

        % tf-idf
        В качестве дополнительной информации, в векторизацию сообщения может
        быть заложена частота встречаемости термов. Для определния такой частоты
        используется мера {\it tf-idf}, которая вычисляется следующим образом:

        \newcommand\tfidf{\mathop{\mbox{$tf$-$idf$}}}
        \begin{equation}
            \label{eq:tfidf}
            \tfidf(t,d,D) = tf(t,d) \cdot idf(t, D)
        \end{equation}

        В формуле \ref{eq:tfidf}, параметр $t$ -- терм, $d$ -- документ, являющийся
        элементом коллекции документов $D$. Функция {\it tf} определяет {\it частоту встречаемости}
        (от англ. {\it term frequency}) терма $t$ в документе $d$ (см. формулу \ref{eq:tf}).
        \begin{equation}
            \label{eq:tf}
            tf(t, d) = \dfrac{n_i}{|d|}
        \end{equation}

        Под {\it idf } мерой понимается {\it инвертированная частота терма} (от англ.
        {\it inverted document frequency}) в коллекции документов, и определяется
        формулой \ref{eq:idf}. Чем больше значение {\it idf(t, d)}, тем выше уровень
        ``уникальности'' рассматриваемого терма $t$ для коллекции документов $D$.
        \begin{equation}
            \label{eq:idf}
            idf(t, D) = log \Bigg[ \dfrac{|D|}{|\{d_i \in D | t_i \in d_i|\}|} \Bigg]
        \end{equation}

        % tf-idf + вспомогательный словарь.
        \subsubsection{Вспомагательные признаки для сообщений}
        \label{sec:additionalFeatures}

        Для повышения качества классификации, вектор каждого сообщения можно дополнить
        вспомогательными признаками. Основная задача, которая ставится при введении
        признаков -- это отделение одного множества классов от другого, при том что
        оба множества не являются пустыми. Примерами таких признаков могут служить:
        \begin{itemize}
            \item Учет {\it эмотиконов} в сообщении: <<:)>>, <<;D>>, <<:(>>, <<((>>, и т.д.;
            \item Учет знаков препинания: <<?>>, <<!>>, <<...>>, и т.д.;
            \item Учет числа слов написанных в верхнем регистре.
            \item Признаки на основе предварительно составленых {\it тональных лексиконов}.
        \end{itemize}

        Признаки, построенные на основе первых трех пунктов, позвяют примерно разделить
        множество неэмоциональных сообщений от эмоциональных.

        Использование вспомогательных признаков на основе тональных лексиконов
        позволяет предварительно произвести оценку сообщения, и тем самым
        <<подсказать>> классификатору наиболее вероятный класс сообщения.
