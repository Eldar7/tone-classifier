\subsection{Разработка тонального класификатора}
    % Архитектура проекта
    % Описать зависимости, а также что требуется реализовать.
    % Для решения задачи сентиментального анализа требуется:
    %   - классификатор (LibSVM)
    %   - модули обработки сообщений (Mystem + тональные префиксы + списки стоп слов)
    %
    % Выбор языка для реализации приложения
    %   - Python
    % хранения коллекций сообщений сети Twitter, а также вспомогательных данных:
    %   - База данных
    В качестве основы для разработки приложения был выбран язык {\it Python},
    ввиду следующих особенностей:
    \begin{enumerate}
        \item Отсутствие временных ограничений на обработку сообщений,
            классификацию;
        \item Возможность быстро подстраиваться под изменение архитектуры
            проекта и введения новых возможностей;
        \item Наличие библиотеки LibSVM реализующей
            классификацию методом <<Опорных Векторов>>.\cite{svmClassifier}
    \end{enumerate}

    Что касается вопроса хранения коллекций и вспомогательной информации
    для решения задачи классификации, для этих целей используется СУБД
    {\it PostgreSQL}. Такой выбор обусловлен удобным интерфейсом взаимодействия
    через сценарии языка {\it Python}, а также возможностью удобного консольного
    администрирования хранилища. Как и в случае с выбором языка для реализации
    задачи, на выбор хранилища не накладываются временные ограничения.

    Под разработкой проекта понимается реализация сценариев на языке Python,
    для выполнения следующих подздачач:
    %
    % Описать в формате потока, какие действия требуются чтобы произвести
    % классификацию с оценкой или без нее.
    %
    \begin{enumerate}
        \item Подготовка коллекций -- импорт XML данных в хранилище;
        \item Построение модели на основе обучающей коллекции;
        \item Применение модели к данным тестовой коллекции;
        \item Экспорт размеченных классификатором сообщений из хранилища в XML формат;
        \item Вычисление оценки качества работы модели.
    \end{enumerate}

    \subsubsection{Импорт/Экспорт сообщений}
    \label{sec:devImporting}
    % Описать используемые поля (про идентификаторы в частности)
    Для хранения исходных коллекций, данные формата \xml экспортировались в
    СУБД {\it PostrgreSQL}.
    В п. \ref{sec:programmingInnerFormat} рассматривается структура данных
    коллекции, с указанием на классы параметров.
    В зависимости от рассматриваемой задачи (см. п. \ref{sec:tonalityCompetition}),
    изменяется только набор дополнительных параметров, а основные остаются
    неизменными.
    Формат таблиц описывается с помощью {\it SQL} синтаксиса (см. п.
    \ref{sec:programmingInnerFormat}).
    В листинге \ref{lst:bankSQL} приведен пример сценария создания таблицы
    для сообщений в банковской сфере.

    % Листинги

    \lstset{language=sql}
    \lstinputlisting[caption="Сценарий создания таблицы для хранения тестовой
        коллекции сообщений банковской сферы",
        label={lst:bankSQL}]{parts/code/dev/bankSQL.sql}

    % Рассказать про использование psycopg
    % Модуль импорта сообщений x2pg_data.py
    Дополнительными полями в листинге \ref{lst:bankSQL} являются те,
    которые записаны в строках 6-13 и указывают на рассматриваемые компании.
    Сценарий создания таблицы для сферы ТКК отличается лишь набором дополнительных
    полей.

    Работа с таблицами {\it PostgreSQL} в языке {\it Python} производится с помощью
    библиотеки {\tt psycopg}.
    Для удобного чтения \xml файла используется библиотека {\tt libxml}.
    Преобразование \xml формата в хранилище рассматривается в листинге
    \ref{lst:importing}.

    \lstset{style=python}
    \lstinputlisting[caption="Импорт данных из формата XML в таблицу PostgreSQL",
        label={lst:importing}]{parts/code/dev/importing.py}

    % Модуль экспорта сообщений pg2x_data.py
    Экспорт сообщений из PostgreSQL в \xml формат (см. листинг
    \ref{lst:collectionMessageExample}) реализован с помощью библиотеки {\tt etree}
    для построения XML дерева.
    В листинге \ref{lst:exporting} приведен процесс построения дерева с помощью
    курсора, созданного для обхода таблицы СУБД.

    \lstset{style=python}
    \lstinputlisting[caption="Экспорт данных из таблицы PostgreSQL XML формат",
        label={lst:exporting}]{parts/code/dev/exporting.py}

    \subsubsection{Обработка сообщений сети Twitter}
    % Описать про модуль обработки.
    % Рассказать про применение тональных префиксов, удаление стоп слов.
    % А также, что все начинается с этапа лемматизации сообщения.
    % Что такое слово? -- последовательность символов, разделенная пробелами.
    Прежде чем перейти к описанию процесса преобразования, введем следующие
    понятия:
    \begin{itemize}
        \item {\bf Слово} --- это последовательность симоволов, которая ограничена
            пробельными символами, либо символами перевода строки.
        \item {\bf Терм} --- слова сообщения, к которым либо была применена
            лемматизация, либо применение лемматизации не требуется.
    \end{itemize}

    % рассказать про модуль в проекте. Описываем msg.py (можно просто предоставить
    % диаграмму UML для одного класса).

    Процесс обработки сообщений начинается с этапа разбиния всех слов сообщения
    на следующие классы (согласно п. \ref{sec:buildingMsgProcessing}):
    \begin{itemize}
        \item Имена пользователей сети \twitter --- термы с префиксом <<@>>;
        \item Хэштеги (от англ. {\it Hashtags}) --- термы с префиксом <<\#>>.
        \item Символы <<Ретвита>> --- термы со значением <<RT>>;
        \item Ссылки на ресурсы сети Интернет --- {\it URL\hspace{1pt}}-адреса;
        \item Основные слова --- множество слов, не вошедших ни в один из четырех
            выше перечисленных классов.
    \end{itemize}

    Код программы, отвечающий за выбор класса, к которому относится каждое из
    слов сообщения, представлен в листинге \ref{lst:classSelection}.

    \lstset{style=python}
    \lstinputlisting[caption="Определение класса для каждого из слов сообщения",
        label={lst:classSelection}]{parts/code/dev/classSelection.py}

    Все слова, не относящиеся ко множеству {\it основных слов}, не требуют
    дальнейшей лемматизации. Поэтому, ввиду выше введенных определений, слова
    этих множеств можно считать термами.

    % Лемматизация
    На следующем этапе обработки сообщения выполняется {\it лемматизация основных слов}.
    Результатом этого этапа должны стать леммы.
    Каждая лемма представляет собой последовательность буквенных символов.
    Другими словами, леммы должны быть очищены от знаков препинания, эмотиконов,
    и т.п.

    Для преобразования слова в лемму, используется пакет {\it Mystem} компании
    {\it Yandex} \cite{mystem}.
    Такой пакет предоставляет одноименный класс {\it Mystem}, объект на основе
    которого используется для решения такой задачи.
    Лемматизация производится на основе словарей русского языка, которые
    добавляются в качестве компонента к пакету при первом его использовании.

    Лемматизация основных слов сообщения приведена в листинге \ref{lst:lemmatization}.
    Для, того чтобы игнорировать знаки препинания, а также эмотиконы которые
    могут встречаться в конце слов, пакет предусматривает в качестве аргумента
    параметр {\it entire\_input} со значением {\it false} (строка 1, листинг
    \ref{lst:lemmatization}).

    \lstset{style=python}
    \lstinputlisting[caption="Лемматизация основных слов сообщения",
        label={lst:lemmatization}]{parts/code/dev/lemmatize.py}

    % Использование стоп слов.
    После того как сообщение преобразовано в список лемм, производится
    {\it опциальное\footnote{
            Разрешение на выполнение этого этапа, как и любых других опциальных,
            указывается в конфигурационных файлах приложения.
        }
    отбрасывание из этого списка стоп-слов}.
    Предусматривается два типа списков:
    \begin{itemize}
        \item Абсолютный списко стоп-слов --- является общим для всех задач.
            (см. п. \ref{sec:tonalityCompetition});
        \item Список стоп-слов конкретной задачи.
    \end{itemize}

    Реализация этой операции приведена в листинге \ref{lst:stopwords}.
    \lstset{style=python}
    \lstinputlisting[caption="Применение списка стоп слов для фильтрации лемм",
        label={lst:stopwords}]{parts/code/dev/stopwords.py}

    % Использование тональных префиксов.
    Последний этап, который также выполняется опционально, реализует замену
    некоторых лемм на тональные префиксы <<+>> и <<->>, а также дальнейшую
    конкатенацию префиксов со следующими термами списка. Тональные префиксы
    могут представлять собой список биграмм и униграмм. В листинге
    \ref{lst:tonePrefixes} представлена реализация такого преобразования.

    \lstset{style=python}
    \lstinputlisting[caption="Преобразование термов в тональные префиксы",
        label={lst:tonePrefixes}]{parts/code/dev/tonePrefixes.py}

    % В приложении нужно добавить список исползуемых СТОП СЛОВ, тональных
    % префиксов, ...

    % Получили список термов. Используем эти термы для обучения классификатора.
    После завершения всех описанных выше этапов выполняется векторизация
    сообщения.

    \subsubsection{Использование LibSVM для классификации методом <<Опорных векторов>>}

    % Цели использования SVM классификатора
    % Здесь еще было бы неплохо сделать привязку к файлам.
    Сообщения в векторизованном виде используются как для обучения
    модели на основе классификатора SVM, так и для тестирования.
    Обучение или применение построенной модели к данным, зависит от типа коллекции,
    которая подверглась векторизации.
    Так, векторизация сообщений обучающей коллекции необходима для построения
    обучающей модели, а векторизация сообщений тестовой --- для разметки сообщений
    на основе уже построенной модели.

    % Какой формат входных данных (для обучения и для классификации).
    % Про обучение
    Рассмотрим формат представления коллекций, с которым рабтает пакет {\it LibSVM}.
    Чтобы создать классификационную модель, сообщения для обучения должны быть
    представлены в формате векторов, которые записываются следующим образом:
    \begin{center}
        \tt
        <label> <$index_1$>:<$value_1$> <$index_2$>:<$value_2$> ...
    \end{center}

    Рассмотрем параметры, входящие в шаблон:
    \begin{itemize}
        \item {\bf Метка} ({\it от англ. Label}) --- применительно к задаче
        тональной классификации, указывает на оценку которая была присвоена
        соотвествующему векторизованному сообщению;
        \item {\bf Индекс терма} --- уникальное числовое значение, используемое
        для идентифицирования соответствующего терма среди всех термов, встречающихся
        в коллекции;
        \item {\bf Значение} ({\it от англ. Value}) --- числовое значение,
        соответсвующее терму.
    \end{itemize}

    % Про применение
    По аналогии с форматом описания данных для обучения классификатора, вектора
    тестовой коллекции описываются аналогичным образом, с той лишь разницей, что
    в качестве {\it <<метки>>} может быть передано любое значение.
    Значение этого параметра не используется при классификации.
    В текущей реализации, этот парамет хранит идентификатор сообщения (поле
    {\it id}, см. п. \ref{sec:devImporting}).

    % Как производилась векторизация (модуль составления общего словаря voc).
    % indexer.py Модуль векторизации сообщений (зависит от хранения данных в бд)
    Поскольку индексы термов тестовой и обучающих коллекций должны быть
    синхронизованы, то индексация должна производится для объединенного
    множества термов каждой из коллекций.
    Модуль генерации индексов работает с коллекциями сообщений сети \twitter,
    представленных в формате таблиц хранилища. В Листинге \ref{lst:indexing}
    реализовано составление словаря на основе списка таблиц с коллекциями.

    \lstset{style=python}
    \lstinputlisting[caption="Получения словаря индексов для всех термов обучающей и тестовой коллекций",
        label={lst:indexing}]{parts/code/dev/indexing.py}

    % Что использовалось в качестве значения.
    Вычисление параметра {\it <<значение>>} производилось следующими способами:
    \begin{enumerate}
        \item На основе метрики {\it tf-idf} (см. формулу \ref{eq:tfidf};
        \item Искуственное повышение точности метрики {\it tf-idf} засчет информации о числе
            вхождении наиболее популярных термов в корпус из 1 миллиона документов.
    \end{enumerate}

    Во втором случае, информация числе документов, в которых содержится тот или
    иной терм, представлена в текстовом файле в формате списка:
    \begin{center}
        \it
        <<терм>> <<число документов, содержащих терм>>
    \end{center}

    Пусть $K$ --- основной корпус документов.
    Обозначим $E$ --- корпус, на основе которого была построена информация листинга
    \ref{lst:extendedFrequency}.
    Тогда, формула вычисления {\it инвертированной меры документов} будет выглядеть
    следующим образом:
    \begin{equation}
        idf(t, K \cup E) = log \Bigg[ \dfrac{|K \cup E|}{|\{d_i \in K \cup E | t_i \in d_i|\}|} \Bigg]
    \end{equation}

    % Запуск тестирования, и сохранение результатов
    % predict.py
    После того, как были подготовлены тестовые и обучающие коллекции к
    описанному выше формату, выполняются действия, за которые
    отвечает {\it модуль классификации} (реализация приведена в листинге
    \ref{lst:testing}):
    \begin{enumerate}
        \item Построение модели на основе обучающих данных (строки 4-5);
        \item Применение построенной модели к тестовой коллекции (строки 6-9).
    \end{enumerate}

    \lstset{style=python}
    \lstinputlisting[caption="Построение модели и ее применение к тестовым данным",
        label={lst:testing}]{parts/code/dev/testing.py}

    % Тестирование (модуль тестирования). Здесь не хватает бы общей архитектуры
    % создать, чтобы можно было в целом понимать как что взаимодействует.
    % Может имеет смысл потоком это отобразить.

    \subsubsection{Дополнительные признаки классификации}
    % Модуль добавления признаков в векторизацию сообщений.
    % Модуль features.py
    Рассмотрим реализацию дополнительных признаков п.
    \ref{sec:buildingAdditionalFeatures}.
    Признаки добавлялись в формате термов к сообщению.
    Такие термы представляли собой ключевые слова, префиксом которых является
    символ {\it <<\$>>}.

    Для обозначения признаков используются следующие ключевые слова (термы):
    \begin{itemize}
        \item {\tt \$emoticons} --- вычисление значения признака на основе
            множеств положительных и негативных эмотиконов.
            Для этого признака используется исходный текст сообщения
            (см. листинг \ref{lst:emoticons});
        \item {\tt \$capitals} --- признак, используемый для указания числа термов,
            записанных в верхнем регистре (см. листинг \ref{lst:capitals});

        \item {\tt \$signs} --- подсчет числа знаков препинания (см. листинг
            \ref{lst:signs}).
    \end{itemize}

    % Списки спользуемых эмотиконов описать в разделе тестирования.
    Ввиду того, что лексиконов может быть исползовано несколько, а также на основе
    каждого лексикона реализуется несколько признаков, ключевое слово
    задается в следующем формате:
    \begin{center}
        \tt
        \$\{имя признака на основе лексикона\}
    \end{center}

    Таким образом, имя признака включает в себя как имя лексикона, так и имя
    функции, на основе которой вычислется значение.
    % Описание введения ключевых слов.
    \lstset{style=python}
    \lstinputlisting[caption="Вычисление значения признака на основе эмотиконов",
        label={lst:emoticons}]{parts/code/dev/emoticons.py}

    \lstset{style=python}
    \lstinputlisting[caption="Учет числа термов записанных в верхнем регистре",
        label={lst:capitals}]{parts/code/dev/capitals.py}

    \lstset{style=python}
    \lstinputlisting[caption="Подсчет числа знаков препинания",
        label={lst:signs}]{parts/code/dev/signs.py}
