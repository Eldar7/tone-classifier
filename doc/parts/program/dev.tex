\subsection{Разработка тонального класификатора}
    % Архитектура проекта
    % Описать зависимости, а также что требуется реализовать.
    % Для решения задачи сентиментального анализа требуется:
    %   - классификатор (LibSVM)
    %   - модули обработки сообщений (Mystem + тональные префиксы + списки стоп слов)
    %
    % Выбор языка для реализации приложения
    %   - Python
    % хранения коллекций сообщений сети Twitter, а также вспомогательных данных:
    %   - База данных
    В качестве основы для разработки приложения был выбран язык {\it Python},
    ввиду следующих особенностей:
    \begin{enumerate}
        \item Отсутствие временных ограничений на обработку сообщений,
            классификацию;
        \item Возможность быстро подстраиваться под изменение архитектуры
            проекта и введения новых возможностей;
        \item Наличие библиотеки LibSVM\cite{svmClassifier} реализующей
            классификацию методом <<Опорных Векторов>>.
    \end{enumerate}

    Что касается вопроса хранения коллекций и вспомогательной информации
    для решения задачи классификации, для этих целей используется СУБД
    {\it PostgreSQL}. Такой выбор обусловлен удобным интерфейсом взаимодействия
    через сценарии языка {\it Python}, а также возможностью удобного консольного
    администрирования хранилища. Как и в случае с выбором языка для реализации
    задачи, на выбор хранилища не накладываются временные ограничения.

    Под разработкой проекта понимается реализация сценариев на языке Python,
    для выполнения следующих подздачач:
    %
    % Описать в формате потока, какие действия требуются чтобы произвести
    % классификацию с оценкой или без нее.
    %
    \begin{enumerate}
        \item Подготовка коллекций -- импорт XML данных в хранилище;
        \item Построение модели на основе обучающей коллекции;
        \item Применение модели к данным тестовой коллекции;
        \item Экспорт размеченных классификатором сообщений из хранилища в XML формат;
        \item Вычисление оценки качества работы модели.
    \end{enumerate}

    \subsubsection{Импорт сообщений в хранилище}
    \label{sec:devImporting}

    \subsubsection{Обработка сообщений сети Twitter}
    % Описать про модуль обработки.
    % Рассказать про применение тональных префиксов, удаление стоп слов.
    % А также, что все начинается с этапа лемматизации сообщения.
    % Что такое слово? -- последовательность символов, разделенная пробелами.
    Прежде чем перейти к описанию процесса преобразования, введем следующие
    понятия:
    \begin{itemize}
        \item {\bf Слово} --- это последовательность симоволов, которая ограничена
            пробельными символами, либо символами перевода строки.
        \item {\bf Терм} --- слова сообщения, к которым либо была применена
            лемматизация, либо применение лемматизации не требуется.
    \end{itemize}

    % рассказать про модуль в проекте. Описываем msg.py (можно просто предоставить
    % диаграмму UML для одного класса).

    Процесс обработки сообщений начинается с этапа разбиния всех слов сообщения
    на следующие классы (согласно п. \ref{sec:buildingMsgProcessing}):
    \begin{itemize}
        \item Имена пользователей сети \twitter --- термы с префиксом <<@>>;
        \item Хэштеги (от англ. {\it Hashtags}) --- термы с префиксом <<\#>>.
        \item Символы <<Ретвита>> --- термы со значением <<RT>>;
        \item Ссылки на ресурсы сети Интернет --- {\it URL\hspace{1pt}}-адреса;
        \item Основные слова --- множество слов, не вошедших ни в один из четырех
            выше перечисленных классов.
    \end{itemize}

    Код программы, отвечающий за выбор класса, к которому относится каждое из
    слов сообщения, представлен в листинге \ref{lst:classSelection}.

    \lstset{style=python}
    \lstinputlisting[caption="Определение класса для каждого из слов сообщения",
        label={lst:classSelection}]{parts/code/dev/classSelection.py}

    Все слова, не относящиеся ко множеству {\it основных слов}, не требуют
    дальнейшей лемматизации. Поэтому, ввиду выше введенных определений, слова
    этих множеств можно считать термами.

    % Лемматизация
    На следующем этапе обработки сообщения выполняется лемматизация основных слов.
    Результатом этого этапа должны стать леммы, представлюящие собой последовательность
    буквенных символов.
    Другими словами, леммы должны быть очищены от знаков препинания, эмотиконов,
    и т.п.

    Для преобразования слова в лемму, используется пакет {\it Mystem} компании
    {\it Yandex} \cite{mystem}.
    Такой пакет предоставляет одноименный класс {\it Mystem}, объект на основе
    которого используется для лемматизации текстовых сообщений.
    Лемматизация производится на основе словарей русского языка, которые
    добавляются в качестве компонента к пакету при первом его использовании.

    Для, того чтобы игнорировать знаки препинания, а также эмотиконы которые
    могут встречаться в конце слов, пакет предусматривает в качестве аргумента
    параметр {\it entire\_input} со значением {\it false}.
    Лемматизация основных слов сообщения приведена в листинге \ref{lst:lemmatization}.

    \lstset{style=python}
    \lstinputlisting[caption="Лемматизация основных слов сообщения",
        label={lst:lemmatization}]{parts/code/dev/lemmatize.py}

    % Получили список термов. Используем эти термы для обучения классификатора.
    После того как список термов получен, следующим шагом производится
    векторизация сообщения.

    \subsubsection{Использование LibSVM для классификации методом <<Опорных векторов>>}

    % Цели использования SVM классификатора
    % Здесь еще было бы неплохо сделать привязку к файлам.
    Сообщения в векторизованном виде используются как для обучения
    модели на основе классификатора SVM, так и для тестирования.
    Обучение или применение построенной модели к данным, зависит от типа коллекции,
    которая подверглась векторизации.
    Так, векторизация сообщений обучающей коллекции необходима для построения
    обучающей модели, а векторизация сообщений тестовой --- для разметки сообщений
    на основе уже построенной модели.

    % Какой формат входных данных (для обучения и для классификации).
    % Про обучение
    Рассмотрим формат представления коллекций, с которым рабтает пакет {\it LibSVM}.
    Чтобы создать классификационную модель, сообщения для обучения должны быть
    представлены в формате векторов, которые записываются следующим образом:
    \begin{center}
        \tt
        <label> <$index_1$>:<$value_1$> <$index_2$>:<$value_2$> ...
    \end{center}

    Рассмотрем параметры, входящие в шаблон:
    \begin{itemize}
        \item {\bf Метка} ({\it от англ. Label}) --- применительно к задаче
        тональной классификации, указывает на оценку которая была присвоена
        соотвествующему векторизованному сообщению;
        \item {\bf Индекс терма} --- уникальное числовое значение, используемое
        для идентифицирования соответствующего терма среди всех термов, встречающихся
        в коллекции;
        \item {\bf Значение} ({\it от англ. Value}) --- числовое значение,
        соответсвующее терму.
    \end{itemize}

    % Про применение
    По аналогии с форматом описания данных для обучения классификатора, вектора
    тестовой коллекции описываются аналогичным образом, с той лишь разницей, что
    в качестве {\it <<метки>>} может быть передано любое значение.
    Значение этого параметра не используется при классификации.
    В текущей реализации, этот парамет хранит идентификатор сообщения.

    % Как производилась векторизация (модуль составления общего словаря voc).
    % indexer.py Модуль векторизации сообщений (зависит от хранения данных в бд)
    Поскольку индексы термов тестовой и обучающих коллекций должны быть
    синхронизованы, то индексация должна производится для объединенного
    множества термов каждой из коллекций.
    Модуль генерации индексов работает с коллекциями сообщений сети \twitter,
    представленных в формате таблиц хранилища. В Листинге \ref{lst:indexing}
    реализовано составление словаря на основе списка таблиц с коллекциями.

    \lstset{style=python}
    \lstinputlisting[caption="Получения словаря индексов для всех термов обучающей и тестовой коллекций",
        label={lst:indexing}]{parts/code/dev/indexing.py}

    % Что использовалось в качестве значения.
    Вычисление параметра {\it <<значение>>} производилось следующими способами:
    \begin{enumerate}
        \item Вычисление значения на основе метрики tf-idf (см. формулу \ref{eq:tfidf};
        \item Искуственное повышение точности метрики tf-idf засчет информации о числе
            вхождении наиболее популярных термов в корпус из 1 миллиона документов.
    \end{enumerate}

    Во втором случае, информация числе документов, в которых содержится тот или
    иной терм, представлена в текстовом файле в формате списка (см. также листинг
    \ref{lst:extendedFrequency}):
    \begin{center}
        \it
        <<терм>> <<число документов, содержащих терм>>
    \end{center}

    \lstinputlisting[caption="Формат файла описывающего частоту вхождений термов в корпус документов",
        label={lst:extendedFrequency}]{parts/code/dev/extendedFrequency.txt}

    Пусть $K$ --- основной корпус документов.
    Обозначим $E$ --- корпус, на основе которого была построена информация листинга
    \ref{lst:extendedFrequency}.
    Тогда, формула вычисления {\it инвертированной меры документов} будет выглядеть
    следующим образом:
    \begin{equation}
        idf(t, K \cup E) = log \Bigg[ \dfrac{|K \cup E|}{|\{d_i \in K \cup E | t_i \in d_i|\}|} \Bigg]
    \end{equation}

    % Запуск процесса тестирования, и сохранение результатов

    % Тестирование (модуль тестирования). Здесь не хватает бы общей архитектуры
    % создать, чтобы можно было в целом понимать как что взаимодействует.
    % Может имеет смысл потоком это отобразить.


    \subsubsection{Вспомогательные признаки классификации}
    % Модуль добавления признаков в векторизацию сообщений.
    % features и его API

\subsection{Разработка вспомогательных инструментов}
% Выбор языка для реализации вспомогательных инструментов приложения
%   - Python
В роли вспомогательных инструментов выступают компоненты, которые в совокупности
решают задачу построения тонального лексикона на основе сообщений сети \twitter.
Реализация сценариев осуществляется посредством языка {\it Python}.

    \subsubsection{Балансировка исходных обучающих коллекций}
    % Описать с уклоном на атоматическую разметку сообщений на тональные классы.
    % (Как это было сделано в статьях)

    \subsubsection{Прием текстовых сообщений сети Twitter}

    \subsubsection{Создание лексиконов методом определения тональности словосочетаний}

