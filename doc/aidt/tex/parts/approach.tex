\section{Описание подхода}
    \label{sec:buildingApproachDescription}
    В области классификации сообщений методами машинного обучения, использование
    {\it SVM} классификатора (в сравнении с {\it Naive Bayes}) обусловлено результатами
    тестирования в \cite{svmAdvantages}, которые показывают преимущество SVM на униграммной
    модели обработки сообщений.\footnote{
        Использование униграммной модели упрощает процесс обработки сообщения с
        точки зрения добавления метаинформации, в том числе и на основе лексиконов.
        В текущем подходе все термы, содержащиеся во всех лексиконах, являются
        униграммами.
    }
    Для построения обучающей модели и предсказания
    тональности на ее основе, используется библиотека LibSVM \cite{svmClassifier}.

    \subsection{Обработка сообщений}
    \label{sec:buildingMsgProcessing}
    % Векторизация, ее параметры
    Процесс обработки сообщений коллекции сообщений состоит из следующих этапов:
    \begin{enumerate}
        \item Лемматизация слов сообщений с целью получения списка термов\footnote{
            Использование пакета Yandex Mystem:
            \url{http://tech.yandex.ru/mystem/}
        };

        \item Из сообщения удаляются следующие термы:
            символы <<Ретвита>> (термы со значением <<RT>>),
            имена пользователей сети {\it Twitter} (термы с префиксом <<@>>).
            Таким образом, помимо слов естественных языков в сообщении остаются
            \#хэштеги и {\it URL\hspace{1pt}}-адреса;
        \item Замена некоторых биграмм и униграмм на тональные префиксы.
            Для выполнения этого этапа, используется предварительно составленый
            список пар\footnote{
                [Ссылка на github.]
            }
            $D_{tone} = {\langle t, s\rangle}$, где $t$ -- терм, а $s$ --
            тональная оценка (<<+>> или <<-->>). На этом этапе, для каждого терма $t_i$
            сообщения $m$, такого что $t_i \in D_{tone}$ выполняется замена на соответствующую
            оценку $s$, которая становится префиксом следующего терма $t_{i+1}$.
            Пример преобразования:
            \begin{center}
                \it
                Сейчас \underline{хорошо} работать, \underline{плохо} было раньше.

                Сейчас +работать, -было раньше.
            \end{center}
        \item Для получения весовых коэффициентов термов предполагается
            использовать меру {\it tf-idf}.
    \end{enumerate}

    \subsection{Вспомогательные признаки классификации}
    \label{sec:buildingAdditionalFeatures}
    % В этот раздел вносим признаки, которые добавлялись к основной векторизации
    Помимо термов, составляющих вектор сообщения, предполагается внести
    следующие признаки:
    \begin{itemize}
        \item На основе эмотиконов: предварительно составляются два множества
        эмотиконов (положительные и отрицательные).
        Для каждого множества определяется $C$ -- суммарное число вхождений его
        элементов в рассматриваемое сообщение.
        Результирующий числовой коэффициент вычисляется по формуле: $C_+ - C_-$;

        \item Подсчет количества термов, записанных в ВЕРХНЕМ РЕГИСТРЕ;

        \item Подсчет числа знаков препинания: <<?>>, <<...>>, <<!>>;

        \item Пусть $L$ -- множество составленных лексиконов. Тогда относительно
            каждого лексикона $l_j \in L$ для сообщения $m$, вычисляется:
            \begin{gather}
                \sum\limits_{i=1}^N l_j(t_i), \text{ где } t_i \in m
            \end{gather}
            Если терм $t_i$ отсутствует в лексиконе, то в качестве коэффициента
            рассматривается $l_j(t_i) = 0$.
            Дополнительно выполняется нормализация полученного значения в
            диапазоне $\left[ -1, 1 \right]$ на основе преобразования:
            \begin{numcases}{}
                s = 1 - e^{-|x|}, x > 0  {\label{eq:norm1}}  \\
                s = - (1 - e^{-|x|}), x < 0 {\label{eq:norm2}}
            \end{numcases}
    \end{itemize}

    \subsection{Коллекции данных для обучения}
    % Здесь рассказываем про коллекции, которые использовались несбалансированные для обучения коллекции
    Для обучения классификатора предполагается использовать соответствующие
    коллекции данных соревнований {\it SentiRuEval}.

    [Вставить таблицу с обучающими коллекциями!]

    Поскольку в предоставляемых
    данных число тональных сообщений существенно уступает объему класса
    нейтральных сообщений, то дополнительно планируется создать {\it сбалансированную
    обучающую коллекцию}.
    В работе \cite{diploma2015}, применительно к классификаторам {\it
    Наивного Байеса} и {\it SVM}, отмечается существенный прирост качества при
    использовании коллекций сбалансированного типа.

    % Про балансировку коллекций в том числе.
    Для решения подобной задачи воспользуемся готовым общедоступным корпусом Ю.~Рубцовой
    \cite{rubtsovaCollection}, в
    котором каждое сообщение автоматически распределено в одну из тональных групп:
    {\it positive} и {\it negative}.
    Характеристики такого корпуса представлены в таблице
    \ref{table:rubtsovaCorpusSpecs}.

    \begin{table}[H]
    \centering
    \caption{Параметры корпуса коротких сообщений сети {\it Twitter}, Ю.Рубцова}
    \label{table:rubtsovaCorpusSpecs}
    \begin{tabular}{|c|c|}
    \hline
    Коллекция & Число сообщений \\ \hline
    {\it positive} & 114\hspace{3pt}991 \\ \hline
    {\it negative} & 111\hspace{3pt}923 \\ \hline
    \end{tabular}
    \end{table}

    % (Как производить балансировку)
    Среди всех сообщений каждого класса корпуса коротких сообщений, необходимо
    отобрать небольшой процент тех, которые являются наиболее эмоциональными.
    В общем случае, можно сказать, что требуется функция, которая бы на основе
    слов сообщения, а также эмоциональных коэффициентов каждого из слов,
    позволила бы оценить рассматриваемое предложение.

    Вычисление эмоциональных коэффициентов можно производить
    с использованием лексиконов, на основе рассмотренного подхода \cite{severyn} (п. \ref{sec:review}).
    Определение оценки сообщения можно произвести одним из следующих способов:
    \begin{itemize}
        \item Вычисление {\it суммы эмоциональных коэффициентов} всех входящих в сообщение слов;
        \item Вычисление {\it максимального коэффициента} среди всех слов сообщения.
    \end{itemize}

    Если полученные значения рассматривать в формате абсолютных величин, то
    введение нижнего порогового значения позволяет отобрать наиболее подходящие
    сообщения. Таким образом можно получить сообщения, которые являются
    наиболее предпочтительными для балансировки обучающих коллекций.
